# TensorFlow Tensorboard

æœ¬æ–‡ä¸»è¦ä»‹ç´¹ TensorFlow çš„ Tensorboard æ¨¡å¡Šã€‚

Tensorboard å¯ä»¥çœ‹åšæ˜¯æˆ‘å€‘æ§‹å»ºçš„Graph çš„å¯è¦–åŒ–å·¥å…·ï¼Œå°æ–¼æˆ‘å€‘åˆå­¸è€…ç†è§£ç¶²çµ¡æ¶æ§‹ã€æ¯å±¤ç¶²çµ¡çš„ç´°ç¯€éƒ½æ˜¯å¾ˆæœ‰å¹«åŠ©çš„ã€‚ç”±æ–¼å‰å¹¾å¤©å‰›æ¥è§¸ TensorFlowï¼Œæ‰€ä»¥åœ¨å˜—è©¦å­¸ç¿’ Tensorboard çš„éç¨‹ä¸­ï¼Œé‡åˆ°äº†ä¸€äº›å•é¡Œã€‚åœ¨æ­¤åŸºç¤ä¸Šï¼Œåƒè€ƒäº† TensorFlow å®˜æ–¹çš„ Tensorboard Tutorials ä»¥åŠç¶²ä¸Šçš„ä¸€äº›æ–‡ç« ã€‚ç”±æ–¼å‰ä¸ä¹… TensorFlow 1.0 å‰›ç™¼ä½ˆï¼Œç¶²ä¸Šçš„ä¸€äº›å­¸ç¿’è³‡æºæˆ–è€…æ˜¯ tensorboard ä»£ç¢¼åœ¨æ–°çš„ç‰ˆæœ¬ä¸­ä¸¦ä¸é©ç”¨ï¼Œæ‰€ä»¥è‡ªå·±æ”¹å¯«å¹¶å¯¦ç¾äº†å®˜æ–¹ç¶²ç«™ä¸ŠæåŠçš„ä¸‰å€‹å¯¦ä¾‹çš„ Tensorboard ç‰ˆæœ¬ ï¼š
1. æœ€åŸºç¤ç°¡å–®çš„ã€Œlinear modelã€
2. åŸºæ–¼ MNIST æ‰‹å¯«é«”æ•¸æ“šé›†çš„ ã€Œsoftmax regressionã€æ¨¡å‹
3. åŸºæ–¼ MNIST æ‰‹å¯«é«”æ•¸æ“šé›†çš„ã€ŒCNNã€æ¨¡å‹

æ–‡ç« ä¸æœƒè©³ç´°ä»‹ç´¹ TensorFlow ä»¥åŠ Tensorboard çš„çŸ¥è­˜ï¼Œä¸»è¦æ˜¯æ¨¡å‹çš„ä»£ç¢¼ä»¥åŠéƒ¨åˆ†æ¨¡å‹å¯¦é©—æˆªåœ–ã€‚

æ³¨æ„ï¼šæ–‡ç« å‰æé»˜èªè®€è€…å€‘çŸ¥æ›‰ TensorFlowï¼ŒçŸ¥æ›‰ Tensorboardï¼Œä»¥åŠ TensorFlow çš„ä¸€äº›ä¸»è¦æ¦‚å¿µã€ŒVariablesã€ã€ã€Œplaceholderã€ã€‚é‚„æœ‰ï¼Œé»˜èªä½ å·²ç¶“å°‡éœ€è¦ç”¨åˆ°çš„ MNIST æ•¸æ“šé›†ä¸‹è¼‰åˆ°äº†ä½ ä»£ç¢¼ç•¶å‰æ‰€åœ¨æ–‡ä»¶å¤¾ã€‚

## Environment

**OS: macOS Sierra 10.12.x**

**Python Version: 3.4.x**

**TensorFlow: 1.0**


## Tensorboard

Tensorboardæœ‰å¹¾å¤§æ¨¡å¡Šï¼š

- SCALARSï¼šè®°å½•å–®ä¸€è®Šé‡çš„ï¼Œä½¿ç”¨ `tf.summary.scalar()` æ”¶é›†æ§‹å»ºã€‚
- IMAGESï¼šæ”¶é›†çš„å›¾ç‰‡æ•°æ®ï¼Œå½“æˆ‘ä»¬ä½¿ç”¨çš„æ•°æ®ä¸ºå›¾ç‰‡æ—¶ï¼ˆé€‰ç”¨ï¼‰ã€‚
- AUDIOï¼šæ”¶é›†çš„éŸ³é¢‘æ•°æ®ï¼Œå½“æˆ‘ä»¬ä½¿ç”¨æ•°æ®ä¸ºéŸ³é¢‘æ—¶ï¼ˆé€‰ç”¨ï¼‰ã€‚
- GRAPHSï¼šæ„ä»¶å›¾ï¼Œæ•ˆæœå›¾ç±»ä¼¼æµç¨‹å›¾ä¸€æ ·ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°æ•°æ®çš„æµå‘ï¼Œä½¿ç”¨`tf.name_scope()`æ”¶é›†æ§‹å»ºã€‚
- DISTRIBUTIONSï¼šç”¨äºæŸ¥çœ‹å˜é‡çš„åˆ†å¸ƒå€¼ï¼Œæ¯”å¦‚ Wï¼ˆWeightsï¼‰å˜åŒ–çš„è¿‡ç¨‹ä¸­ï¼Œä¸»è¦æ˜¯åœ¨ 0.5 é™„è¿‘å¾˜å¾Šã€‚
- HISTOGRAMSï¼šç”¨äºè®°å½•å˜é‡çš„å†å²å€¼ï¼ˆæ¯”å¦‚ weights å€¼ï¼Œå¹³å‡å€¼ç­‰ï¼‰ï¼Œå¹¶ä½¿ç”¨æŠ˜çº¿å›¾çš„æ–¹å¼å±•ç°ï¼Œä½¿ç”¨`tf.summary.histogram()`è¿›è¡Œæ”¶é›†æ§‹å»ºã€‚

## Examples

- æœ€ç°¡å–®çš„ç·šæ€§å›æ­¸æ¨¡å‹ï¼ˆtensorboard ç¹ªåœ–ï¼‰

```python
import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt

def add_layer(layoutname, inputs, in_size, out_size, act = None):
	with tf.name_scope(layoutname):
		with tf.name_scope('weights'):
			weights = tf.Variable(tf.random_normal([in_size, out_size]), name = 'weights')
			w_hist = tf.summary.histogram('weights', weights)
		with tf.name_scope('biases'):
			biases = tf.Variable(tf.zeros([1, out_size]) + 0.1, name = 'biases')
			b_hist = tf.summary.histogram('biases', biases)
		with tf.name_scope('Wx_plus_b'):
			Wx_plus_b = tf.add(tf.matmul(inputs, weights), biases)

		if act is None:
			outputs = Wx_plus_b
		else :
			outputs = act(Wx_plus_b)
		return outputs


x_data = np.linspace(-1, 1, 300)[:,np.newaxis]
noise = np.random.normal(0,0.05, x_data.shape)
y_data = np.square(x_data) - 0.5 + noise

with tf.name_scope('Input'):
	xs = tf.placeholder(tf.float32, [None, 1], name = "input_x")
	ys = tf.placeholder(tf.float32, [None, 1], name = "target_y")


l1 = add_layer("first_layer", xs, 1, 10, act = tf.nn.relu)
l1_hist = tf.summary.histogram('l1', l1)

y = add_layer("second_layout", l1, 10, 1, act = None)
y_hist = tf.summary.histogram('y', y)

with tf.name_scope('loss'): 
	loss = tf.reduce_mean(tf.reduce_sum(tf.square(ys - y), 
							reduction_indices = [1]))
	tf.summary.histogram('loss ', loss)
	tf.summary.scalar('loss', loss)

with tf.name_scope('train'):
	train_step = tf.train.GradientDescentOptimizer(0.1).minimize(loss)

init = tf.global_variables_initializer()
merged = tf.summary.merge_all()

with tf.Session() as sess:
	fig = plt.figure()
	ax = fig.add_subplot(1, 1, 1)
	ax.scatter(x_data, y_data)
	plt.ion()
	plt.show()
	
	writer = tf.summary.FileWriter('logs/', sess.graph)
	sess.run(init)
	
	for train in range(1000):
		sess.run(train_step, feed_dict = {xs: x_data, ys: y_data})
		if train % 50 == 0:
			try:
				ax.lines.remove(lines[0])
			except Exception:
				pass
			summary_str = sess.run(merged, feed_dict = {xs: x_data, ys: y_data})
			writer.add_summary(summary_str, train)

			print(train, sess.run(loss, feed_dict = {xs: x_data, ys: y_data}))
			
			prediction_value = sess.run(y, feed_dict = {xs: x_data})
			lines = ax.plot(x_data, prediction_value, 'r-', lw = 5)
			plt.pause(1)
```

- åŸºæ–¼ Softmax Regressions çš„ MNIST æ•¸æ“šé›†ï¼ˆtensorboard ç¹ªåœ–ï¼‰

```python
from tensorflow.examples.tutorials.mnist import input_data
import tensorflow as tf
import numpy as np

def add_layer(layoutname, inputs, in_size, out_size, act = None):
	with tf.name_scope(layoutname):
		with tf.name_scope('weights'):
			weights = tf.Variable(tf.zeros([in_size, out_size]), name = 'weights')
			w_hist = tf.summary.histogram("weights", weights)
		with tf.name_scope('biases'):
			biases = tf.Variable(tf.zeros(out_size), name = 'biases')
			b_hist = tf.summary.histogram("biases", biases)
		with tf.name_scope('Wx_plus_b'):
			Wx_plus_b = tf.add(tf.matmul(inputs, weights), biases)
		
		if act is None:
			outputs = Wx_plus_b
		else:
			outputs = act(Wx_plus_b)
		return outputs
		
# Import data
mnist_data_path = 'MNIST_data/'
mnist = input_data.read_data_sets(mnist_data_path, one_hot = True)

with tf.name_scope('Input'):
	x = tf.placeholder(tf.float32, [None, 28 * 28], name = 'input_x')
	y_ = tf.placeholder(tf.float32, [None, 10], name = 'target_y')

y = add_layer("hidden_layout", x, 28*28, 10, act = tf.nn.softmax)
y_hist = tf.summary.histogram('y', y)

# labels çœŸå®å€¼ logits é¢„æµ‹å€¼
with tf.name_scope('loss'):
	cross_entroy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels = y_,
					logits = y))
	tf.summary.histogram('cross entropy', cross_entroy)
	tf.summary.scalar('cross entropy', cross_entroy)

with tf.name_scope('train'):
	train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entroy)

# Test trained model
with tf.name_scope('test'):
	correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))
	accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))
	tf.summary.scalar('accuracy', accuracy)

init = tf.global_variables_initializer()
merged = tf.summary.merge_all()

with tf.Session() as sess:
	#logpath = r'/Users/randolph/PycharmProjects/TensorFlow/logs'
	writer = tf.summary.FileWriter('logs/', sess.graph)
	sess.run(init)

	for i in range(1000):
		if i % 10 == 0:
			feed = {x: mnist.test.images, y_: mnist.test.labels}
			result = sess.run([merged, accuracy], feed_dict = feed)
			summary_str = result[0]
			acc = result[1]
			writer.add_summary(summary_str, i)
			print(i, acc)
		else:
			batch_xs, batch_ys = mnist.train.next_batch(100)
			feed = {x: batch_xs, y_: batch_ys}
			sess.run(train_step, feed_dict = feed)

	print('final result: ', sess.run(accuracy, feed_dict = {x: mnist.test.images, y_: mnist.test.labels}))
```

- åŸºæ–¼ CNN çš„ MNIST æ•¸æ“šé›†ï¼ˆtensorboard ç¹ªåœ–ï¼‰

```python
from tensorflow.examples.tutorials.mnist import input_data
import tensorflow as tf
import numpy as np

def weight_variable(shape):
	initial = tf.truncated_normal(shape, stddev = 0.1)
	return tf.Variable(initial)
	
def bias_variable(shape):
	initial = tf.constant(0.1, shape = shape)
	return tf.Variable(initial)

def conv2d(x, W):
	return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')

def max_pool_2x2(x):
	return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')
	
def variable_summaries(var):
	"""Attach a lot of summaries to a Tensor (for TensorBoard visualization)."""
	with tf.name_scope('summaries'):
		mean = tf.reduce_mean(var)
		tf.summary.scalar('mean', mean)
		with tf.name_scope('stddev'):
			stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))
		tf.summary.scalar('stddev', stddev)
		tf.summary.scalar('max', tf.reduce_max(var))
		tf.summary.scalar('min', tf.reduce_min(var))
		tf.summary.histogram('histogram', var)

def add_layer(input_tensor, weights_shape, biases_shape, layer_name, act = tf.nn.relu, flag = 1):
	"""Reusable code for making a simple neural net layer.

	It does a matrix multiply, bias add, and then uses relu to nonlinearize.
	It also sets up name scoping so that the resultant graph is easy to read,
	and adds a number of summary ops.
	"""
	with tf.name_scope(layer_name):
		with tf.name_scope('weights'):
			weights = weight_variable(weights_shape)
			variable_summaries(weights)
		with tf.name_scope('biases'):
			biases = bias_variable(biases_shape)
			variable_summaries(biases)
		with tf.name_scope('Wx_plus_b'):
			if flag == 1:
				preactivate = tf.add(conv2d(input_tensor, weights), biases)
			else:
				preactivate = tf.add(tf.matmul(input_tensor, weights), biases)
			tf.summary.histogram('pre_activations', preactivate)
		if act == None:
			outputs = preactivate
		else:
			outputs = act(preactivate, name = 'activation')
			tf.summary.histogram('activation', outputs)
		return outputs

def main():
	# Import data
	mnist_data_path = 'MNIST_data/'
	mnist = input_data.read_data_sets(mnist_data_path, one_hot = True)
	
	with tf.name_scope('Input'):
		x = tf.placeholder(tf.float32, [None, 28*28], name = 'input_x')
		y_ = tf.placeholder(tf.float32, [None, 10], name = 'target_y')

	# First Convolutional Layer
	x_image = tf.reshape(x, [-1, 28, 28 ,1])
	conv_1 = add_layer(x_image, [5, 5, 1, 32], [32], 'First_Convolutional_Layer', flag = 1)
	
	# First Pooling Layer
	pool_1 = max_pool_2x2(conv_1)
	
	# Second Convolutional Layer 
	conv_2 = add_layer(pool_1, [5, 5, 32, 64], [64], 'Second_Convolutional_Layer', flag = 1)

	# Second Pooling Layer 
	pool_2 = max_pool_2x2(conv_2)

	# Densely Connected Layer
	pool_2_flat = tf.reshape(pool_2, [-1, 7*7*64])
	dc_1 = add_layer(pool_2_flat, [7*7*64, 1024], [1024], 'Densely_Connected_Layer', flag = 0) 
	
	# Dropout
	keep_prob = tf.placeholder(tf.float32)
	dc_1_drop = tf.nn.dropout(dc_1, keep_prob)
	
	# Readout Layer
	y = add_layer(dc_1_drop, [1024, 10], [10], 'Readout_Layer', flag = 0)
	
	# Optimizer
	with tf.name_scope('cross_entroy'):
		cross_entroy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels = y_,
						logits = y))
		tf.summary.scalar('cross_entropy', cross_entroy)
		tf.summary.histogram('cross_entropy', cross_entroy)
	
	# Train
	with tf.name_scope('train'):
		train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entroy)
	
	with tf.name_scope('accuracy'):
		with tf.name_scope('correct_prediction'):
			correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))
		with tf.name_scope('accuracy'):
			accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))
		tf.summary.scalar('accuracy', accuracy)
		
	sess = tf.InteractiveSession()
	merged = tf.summary.merge_all()
	train_writer = tf.summary.FileWriter('train/', sess.graph)
	test_writer = tf.summary.FileWriter('test/')
	tf.global_variables_initializer().run()

	def feed_dict(train):
		if train:
			batch_xs, batch_ys = mnist.train.next_batch(100)
			k = 0.5
		else:
			batch_xs, batch_ys = mnist.test.images, mnist.test.labels
			k = 1.0
		return {x: batch_xs, y_: batch_ys, keep_prob: k}
		
	for i in range(20000):
		if i % 100 == 0:
			# Test
			summary, acc = sess.run([merged, accuracy], feed_dict = feed_dict(False))
			test_writer.add_summary(summary, i)
			print("step %d, training accuracy %g" %(i, acc))
		else:
			# Train
			summary, _ = sess.run([merged, train_step], feed_dict = feed_dict(True))
			train_writer.add_summary(summary, i)			

main()
```
å¯èƒ½å°æ–¼æœ€å¾Œä¸€å€‹æ¨¡å‹ CNN çš„ä»£ç¢¼ï¼Œéœ€è¦ä¸€äº› CNN å·ç©ç¥ç¶“ç¶²çµ¡çš„ä¸€äº›çŸ¥è­˜ã€‚ä¾‹å¦‚ä»€éº¼æ˜¯å·ç©ã€æ± åŒ–ï¼Œé‚„éœ€è¦äº†è§£ TensorFlow ä¸­ç”¨åˆ°çš„ç›¸æ‡‰å‡½æ•¸ï¼Œä¾‹å¦‚`tf.nn.conv2d()`ï¼Œ`tf.nn.max_pool()`ï¼Œé€™è£¡ä¸å†è´…è¿°ã€‚

è²¼ä¸Šæœ€å¾Œä¸€å€‹æ¨¡å‹çš„éƒ¨åˆ†æˆªåœ–ï¼š

- ä»£ç¢¼éƒ¨åˆ†ï¼š

![](https://farm4.staticflickr.com/3813/33035149741_c90aa2c7a7_o.png)

èªªæ˜ï¼šå³é‚Šæ˜¯ CNN ç¶²çµ¡è¨“ç·´çš„æ­¥æ•¸ä»¥åŠå°æ‡‰çš„çµæœï¼Œç´°å¿ƒçš„åŒå­¸å¯èƒ½ç™¼ç¾äº†ï¼Œé€™å€‹ç¨‹åºè·‘äº†æˆ‘æ¥è¿‘åå…­å€‹å°æ™‚ï¼ˆä¸çŸ¥é“æ­£ç¢ºä¸æ­£ç¢ºï¼‰ğŸ˜‚ã€‚ä½†æ˜¯æ˜¨å¤©æ™šä¸Šæˆ‘æ‡‰è©²æ˜¯ï¼Œåé»é–‹å§‹è·‘é€™å€‹ç¨‹åºï¼Œæ›åœ¨å¯¦é©—å®¤ï¼Œæ—©ä¸Šåé»éä¾†çœ‹å®Œæˆäº†ã€‚ç¸½ä¹‹ï¼Œä½ å€‘å¯ä»¥ä¿®æ”¹é‚£å€‹ range(20000)ï¼Œè«‹é‡åŠ›è€Œç‚ºã€‚

---

ä¸Šè¿°ä»£ç¢¼é‹è¡Œå®Œæˆä¹‹å¾Œï¼Œå‘½ä»¤è¡Œä¸­è·³è½‰åˆ°ä»£ç¢¼ç”Ÿæˆçš„ã€Œtrainã€æ–‡ä»¶å¤¾ä¸­ï¼ˆå…¶å’Œä»£ç¢¼æ–‡ä»¶å­˜åœ¨äºåŒä¸€æ–‡ä»¶å¤¾ä¸­ï¼‰ï¼Œç„¶å¾Œè¼¸å…¥ `tensorboard --logdir .`ï¼Œç­‰å¾…ç¨‹åºåæ‡‰ä¹‹å¾Œï¼Œç€è¦½å™¨è¨ªå•`localhost:6006`ï¼ˆç•¶ç„¶ä½ ä¹Ÿå¯ä»¥è‡ªå·±å®šç¾©ç«¯å£ï¼‰ã€‚å¦‚æœä¸å‡ºæ„å¤–ï¼Œä½ æœƒå¾—åˆ°ä»¥ä¸‹å…§å®¹ï¼š

- Scalars:

  ![](https://farm3.staticflickr.com/2524/33035142071_0bfc4e428c_o.png)

- Graphs:

  ![](https://farm1.staticflickr.com/668/33035146431_d86b30092d_o.png)

- Distributions:

  ![](https://farm4.staticflickr.com/3938/33035148401_377afc152d_o.png)

- Histograms:

  ![](https://farm3.staticflickr.com/2943/33035143981_cfa43b9962_o.png)

é—œæ–¼å„å€‹æ¨¡å¡Šçš„ä½œç”¨ï¼Œä»¥åŠå„å€‹è®Šé‡çš„æ„ç¾©ï¼Œæˆ‘åœ¨æ­¤å°±ä¸å†è´…è¿°äº†ã€‚